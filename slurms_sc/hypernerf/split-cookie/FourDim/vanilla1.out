nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0
Thu Apr 18 14:22:07 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A4000    On   | 00000000:31:00.0 Off |                  Off |
| 41%   36C    P8    17W / 140W |      1MiB / 16376MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
/orion/u/yiqingl/envs/gaufre/bin/python
/orion/u/yiqingl/envs/gaufre/bin/pip
True
2.2.1+cu118
Using /sailhome/yiqingl/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/yiqingl/.cache/torch_extensions/py39_cu118/diff_gaussian_rasterization/build.ninja...
Building extension module diff_gaussian_rasterization...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module diff_gaussian_rasterization...
Using /sailhome/yiqingl/.cache/torch_extensions/py39_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/yiqingl/.cache/torch_extensions/py39_cu118/diff_gaussian_rasterization_4dch9/build.ninja...
Building extension module diff_gaussian_rasterization_4dch9...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module diff_gaussian_rasterization_4dch9...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: lynl7130. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.5
wandb: Run data is saved locally in ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1/wandb/run-20240418_142221-dc2lnj4l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FourDim_vanilla1_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lynl7130/GaussianDiff_hypernerf
wandb: üöÄ View run at https://wandb.ai/lynl7130/GaussianDiff_hypernerf/runs/dc2lnj4l/workspace
output path: ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1
Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]
Loading model from: /orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth
0it [00:00, ?it/s]135it [00:00, 45060.56it/s]
Restoring states from the checkpoint path at ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1/checkpoints/last.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1/checkpoints/last.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
Number of points at initialisation :  20075
Testing: |          | 0/? [00:00<?, ?it/s]Saving Results based on checkpoint: ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1/checkpoints/last.ckpt
Testing:   0%|          | 0/134 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/134 [00:00<?, ?it/s]Testing DataLoader 0:   1%|          | 1/134 [00:01<02:33,  0.87it/s]Traceback (most recent call last):
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 794, in _test_impl
    results = self._run(model, ckpt_path=ckpt_path)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 987, in _run
    results = self._run_stage()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
    return self._evaluation_loop.run()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 135, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/loops/evaluation_loop.py", line 396, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py", line 425, in test_step
    return self.lightning_module.test_step(*args, **kwargs)
  File "/orion/u/yiqingl/GaussianDiff/src/models/GS3d.py", line 1616, in test_step
    torch.cuda.synchronize()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/cuda/__init__.py", line 801, in synchronize
    return torch._C._cuda_synchronize()
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/orion/u/yiqingl/GaussianDiff/main.py", line 6, in <module>
    cli = MyLightningCLI(
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/cli.py", line 388, in __init__
    self._run_subcommand(self.subcommand)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/cli.py", line 679, in _run_subcommand
    fn(**fn_kwargs)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 754, in test
    return call._call_and_handle_interrupt(
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 68, in _call_and_handle_interrupt
    trainer._teardown()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1010, in _teardown
    self.strategy.teardown()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/strategies/ddp.py", line 419, in teardown
    super().teardown()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/strategies/parallel.py", line 133, in teardown
    super().teardown()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/pytorch/strategies/strategy.py", line 537, in teardown
    self.lightning_module.cpu()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 82, in cpu
    return super().cpu()
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/nn/modules/module.py", line 960, in cpu
    return self._apply(lambda t: t.cpu())
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/nn/modules/module.py", line 802, in _apply
    module._apply(fn)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/nn/modules/module.py", line 849, in _apply
    self._buffers[key] = fn(buf)
  File "/orion/u/yiqingl/envs/gaufre/lib/python3.9/site-packages/torch/nn/modules/module.py", line 960, in <lambda>
    return self._apply(lambda t: t.cpu())
RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

wandb: - 0.011 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.027 MB uploadedwandb: üöÄ View run FourDim_vanilla1_test at: https://wandb.ai/lynl7130/GaussianDiff_hypernerf/runs/dc2lnj4l/workspace
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./output/hypernerf/split-cookie/FourDim/FourDim_vanilla1/wandb/run-20240418_142221-dc2lnj4l/logs
